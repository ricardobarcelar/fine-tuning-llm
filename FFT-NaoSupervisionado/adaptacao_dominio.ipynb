{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a38fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRIA O AMBIENTE VIRTUAL\n",
    "\n",
    "conda create -n FFT_NaoSupervisionado python=3.11\n",
    "conda activate FFT_NaoSupervisionado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22f3170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTALA AS DEPENDENCIAS\n",
    "\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "pip install transformers==4.41.2 datasets==2.19.1 accelerate==0.30.1 tokenizers==0.19.1\n",
    "\n",
    "pip install ipykernel\n",
    "pip install bitsandbytes\n",
    "python -m ipykernel install --user --name=FFT_NaoSupervisionado --display-name=\"FFT_NaoSupervisionado\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b856c615",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ricardo/.conda/envs/FFT_NaoSupervisionado/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# IMPORTA AS BIBLIOTECAS\n",
    "\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "337805ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset carregado com sucesso:\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 812\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# CONFIGURA OS PARÂMETROS E CARREGA O DATASET\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "DATASET_FILE = \"corpus_juridico.txt\"\n",
    "OUTPUT_DIR = \"./modelo_juridico_adaptado_v2\"\n",
    "\n",
    "raw_datasets = load_dataset('text', data_files={'train': DATASET_FILE})\n",
    "print(\"Dataset carregado com sucesso:\")\n",
    "print(raw_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3642295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo e o tokenizador carregados: Qwen/Qwen2.5-0.5B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# CARREGAR MODELO E TOKENIZADOR\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Muitos modelos causais não têm um pad_token. Usar o eos_token como substituto\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "print(f\"Modelo e o tokenizador carregados: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdf0dc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento será feito em 93 blocos de 256 tokens.\n"
     ]
    }
   ],
   "source": [
    "# PREPARA OS DADOS PARA TREINAMENTO\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # O tokenizador converte o texto em uma sequência de IDs numéricos.\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=False)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,            # Processa múltiplos exemplos de uma vez.\n",
    "    num_proc=os.cpu_count(), # Usa todos os núcleos de CPU disponíveis para acelerar.\n",
    "    remove_columns=[\"text\"]  # Remove a coluna de texto original, pois não é mais necessária.\n",
    ")\n",
    "\n",
    "\n",
    "block_size = 256 # Tamanho do bloco de tokens para treinamento (256, 512, 1024). GPUs com menos memória, usar valor menor.\n",
    "\n",
    "# Agrupa os textos tokenizados em blocos de tamanho fixo.\n",
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Para modelagem de linguagem, os labels (o que o modelo deve prever) são os próprios inputs.\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=os.cpu_count(),\n",
    ")\n",
    "\n",
    "print(f\"Treinamento será feito em {len(lm_datasets['train'])} blocos de {block_size} tokens.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04aa784b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ricardo/.conda/envs/FFT_NaoSupervisionado/lib/python3.11/site-packages/accelerate/accelerator.py:479: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando o fine-tuning de adaptação de domínio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 20/120 [00:15<01:14,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5781, 'grad_norm': 11.455544471740723, 'learning_rate': 1.8e-05, 'epoch': 1.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 40/120 [00:29<00:58,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4561, 'grad_norm': 9.644211769104004, 'learning_rate': 3.8e-05, 'epoch': 3.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 60/120 [00:44<00:40,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6897, 'grad_norm': 12.958475112915039, 'learning_rate': 4.8405871765993433e-05, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 80/120 [00:58<00:29,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2517, 'grad_norm': 6.355950832366943, 'learning_rate': 3.272542485937369e-05, 'epoch': 6.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 100/120 [01:13<00:14,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0729, 'grad_norm': 3.6006433963775635, 'learning_rate': 1.122757546369744e-05, 'epoch': 8.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [01:28<00:00,  1.36it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0175, 'grad_norm': 3.689345121383667, 'learning_rate': 1.006426501190233e-07, 'epoch': 10.0}\n",
      "{'train_runtime': 88.2887, 'train_samples_per_second': 10.534, 'train_steps_per_second': 1.359, 'train_loss': 0.8443330151339372, 'epoch': 10.0}\n",
      "Treinamento concluído. Modelo final gravado em: ./modelo_juridico_adaptado_v2\n",
      "Treinamento concluído. Modelo final gravado em: ./modelo_juridico_adaptado_v2\n"
     ]
    }
   ],
   "source": [
    "# EXECUTA O TREINAMENTO\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False) # Responsável por criar os lotes (batches)\n",
    "\n",
    "\n",
    "# Argumentos de treinamento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "\n",
    "    # Hiperparâmetros do Treinamento\n",
    "    num_train_epochs=10.0,              # Número de vezes que o modelo verá o dataset completo.\n",
    "    per_device_train_batch_size=8,      # Número de exemplos por lote. Ajuste conforme a VRAM da sua GPU.\n",
    "    optim=\"paged_adamw_8bit\",           # Usar o otimizador AdamW de 8-bits para economia massiva de VRAM.\n",
    "    \n",
    "    # Otimizador e Taxa de Aprendizado\n",
    "    learning_rate=5e-5,                 # Taxa de aprendizado inicial (padrão robusto para fine-tuning).\n",
    "    weight_decay=0.01,                  # Regularização L2 para evitar overfitting.\n",
    "    lr_scheduler_type='cosine',         # Estratégia de decaimento da taxa de aprendizado.\n",
    "    warmup_steps=50,                    # Passos de aquecimento para estabilizar o início do treino.\n",
    "\n",
    "    # Salvamento e Checkpoints\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,                     # Salvar um checkpoint a cada 200 passos.\n",
    "    save_total_limit=3,                 # Manter no máximo 3 checkpoints no disco.\n",
    "\n",
    "    # Logs e Performance\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=20,                   # Exibir a perda (loss) a cada 20 passos.\n",
    "    fp16=torch.cuda.is_available(),     # Usar precisão mista (16-bit) se houver GPU NVIDIA compatível.\n",
    ")\n",
    "\n",
    "# Instancia o Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Inicia o fine-tuning!\n",
    "print(\"Iniciando o fine-tuning de adaptação de domínio...\")\n",
    "trainer.train()\n",
    "\n",
    "# Salva o modelo final e o tokenizador\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"Treinamento concluído. Modelo final gravado em: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce8a7bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [TESTE 1] Modelo Original: Qwen/Qwen2.5-0.5B-Instruct ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste 3 aspectos legais da colaboração premiada. Listar 3 aspectos legais da colaboração premiada, de acordo com a lista fornecida:\n",
      "\n",
      "1. Legitimidade da premiação: O prêmio é uma forma de reconhecimento e valorização do trabalho realizado por um indivíduo ou equipe.\n",
      "\n",
      "2. Inclusão de indivíduos com deficiência: As empresas podem conceder prêmios para indivíduos com deficiência, garantindo que todos sejam representados em\n",
      "\n",
      "--- [TESTE 2] Modelo Adaptado (Pós-Fine-tuning): ./modelo_juridico_adaptado_v2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste 3 aspectos legais da colaboração premiada.A colaboração premiada é uma forma de justiça penal consensual sujeita a limites constitucionais.O instituto reflete o movimento de transformação do processo penal em um sistema cooperativo.A aplicação da colaboração premiada deve respeitar os princípios da legalidade e da moralidade administrativa.A proteção de dados e informações pessoais do colaborador é condição para a integridade do procedimento.A colaboração\n"
     ]
    }
   ],
   "source": [
    "# VERIFICAÇÃO E DEMONSTRAÇÃO\n",
    "\n",
    "# Um prompt para testar a \"personalidade\" jurídica do modelo\n",
    "prompt = \"Liste 3 aspectos legais da colaboração premiada.\"\n",
    "\n",
    "print(f\"\\n--- [TESTE 1] Modelo Original: {MODEL_NAME} ---\")\n",
    "generator_original = pipeline('text-generation', model=MODEL_NAME, device=0 if torch.cuda.is_available() else -1)\n",
    "print(generator_original(prompt, max_new_tokens=100, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)[0]['generated_text'])\n",
    "\n",
    "print(f\"\\n--- [TESTE 2] Modelo Adaptado (Pós-Fine-tuning): {OUTPUT_DIR} ---\")\n",
    "generator_adaptado = pipeline('text-generation', model=OUTPUT_DIR, device=0 if torch.cuda.is_available() else -1)\n",
    "print(generator_adaptado(prompt, max_new_tokens=100, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)[0]['generated_text'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FFT_NaoSupervisionado",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
