{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af62eca-d7fc-4217-b6fc-677aab56c6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRIA O AMBIENTE VIRTUAL\n",
    "\n",
    "conda create -n FT_DPO python=3.11\n",
    "conda activate FT_DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0bcf8e-863b-4be5-87d8-bed250b08223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTALA AS DEPENDÊNCIAS\n",
    "\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "pip install transformers==4.42.4 datasets==2.19.1 accelerate==0.30.1 tokenizers==0.19.1 peft==0.10.0 bitsandbytes==0.43.1 trl==0.9.4\n",
    "\n",
    "pip install ipykernel matplotlib\n",
    "python -m ipykernel install --user --name=FT_DPO --display-name=\"FT_DPO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c95c43-9376-427f-b4f4-ecb9274b4f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm as notebook_tqdm\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import DPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19b74fca-5ee0-4483-8fc9-82d162b07286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURAÇÕES E DATASET\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "DATASET_FILE = \"dataset_preferencias_juridico.jsonl\" \n",
    "OUTPUT_DIR = \"./modelo_juridico_dpo\"\n",
    "\n",
    "train_dataset = load_dataset('json', data_files={'train': DATASET_FILE}, split='train')\n",
    "print(\"Dataset de preferências carregado:\")\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c037ae2a-86de-411a-b584-a87aa2bfd4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARREGAMENTO DO MODELO E TOKENIZADOR\n",
    "# >> Observe que neste exemplo estamos usando quantização para dar outro exemplo usando o ecossistema huggingface\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa99943-0454-44d1-bca8-184ee3f4dced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURAÇÃO DO LoRA\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4579eb30-6920-4d31-ab67-3bc8909df1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURAÇÃO E EXECUÇÃO DO TREINAMENTO DPO\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    "    bf16=True,\n",
    "    overwrite_output_dir=True,\n",
    ")\n",
    "\n",
    "# Instanciar o DPOTrainer\n",
    "# Espera as colunas 'prompt', 'chosen', e 'rejected' por padrão.\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=peft_config,\n",
    "    beta=0.1, # Parâmetro de regularização do DPO\n",
    ")\n",
    "\n",
    "print(\"\\nIniciando o treinamento com DPO...\")\n",
    "dpo_trainer.train()\n",
    "\n",
    "print(f\"\\nTreinamento concluído. Salvando o modelo em: {OUTPUT_DIR}\")\n",
    "dpo_trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed7c988-1de0-485b-b002-3512754472ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PEFT_LoRA",
   "language": "python",
   "name": "peft_lora"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
