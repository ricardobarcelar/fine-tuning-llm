{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2695ba7e-cad6-4782-926e-8aad9e4d79e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRIA O AMBIENTE VIRTUAL\n",
    "\n",
    "conda create -n PEFT_PromptTuning python=3.11\n",
    "conda activate PEFT_PromptTuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3092ab4f-e403-464d-bf4d-85074f5e4f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTALA AS DEPENDENCIAS\n",
    "\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "pip install tqdm ipykernel ipywidgets transformers==4.44 datasets==2.19.1 peft==0.12.0 trl==0.9.4\n",
    "# pip installaccelerate==0.30.1 tokenizers==0.19.1 bitsandbytes==0.43.1\n",
    "\n",
    "python -m ipykernel install --user --name=PEFT_PromptTuning --display-name=\"PEFT_PromptTuning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8935f78b-71a9-42d6-953d-94735cc27f41",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# VERIFICAÇÃO DO DATASET\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"dataset_classificacao_juridica.jsonl\", lines=True)\n",
    "print(df['area'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef809d34-2685-49c8-90a2-ccbf2a3c5c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve bug com progress bar no Jupyter\n",
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
    "os.environ[\"TQDM_DISABLE\"] = \"1\"\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e48debdd-8056-43c2-b416-61410a22a4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTA AS BIBLIOTECAS\n",
    "\n",
    "import re, math, torch\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import (\n",
    "    PromptTuningConfig,\n",
    "    PromptTuningInit,\n",
    "    TaskType,\n",
    "    get_peft_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4263dd8-b139-4a58-8881-dc94f34c672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURAÇÕES\n",
    "\n",
    "MODEL_NAME   = \"Qwen/Qwen2.5-0.5B\"\n",
    "DATASET_FILE = \"dataset_classificacao_juridica.jsonl\"\n",
    "OUTPUT_DIR   = \"./modelo_juridico_prompt_tuning\"\n",
    "\n",
    "TEXT_COL  = \"texto\"\n",
    "LABEL_COL = \"area\"\n",
    "\n",
    "NUM_VTOK   = 20          # nº de tokens virtuais (ajuste entre 10 e 50)\n",
    "MAX_LEN_IN = 768         # comprimento máx. do prompt\n",
    "MAX_LEN_Y  = 16          # comprimento máx. do rótulo (curto)\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS     = 5\n",
    "LR         = 5e-3        # LR maior típico de prompt tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0a0536b-5b70-434e-9ec4-8ac209fb38b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset carregado e dividido:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['texto', 'area'],\n",
      "        num_rows: 817\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['texto', 'area'],\n",
      "        num_rows: 102\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['texto', 'area'],\n",
      "        num_rows: 103\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# CARREGAR DATASET\n",
    "full_dataset = load_dataset(\"json\", data_files={\"train\": DATASET_FILE}, split=\"train\")\n",
    "\n",
    "# split: 80/10/10\n",
    "train_test_split = full_dataset.train_test_split(test_size=0.2, shuffle=True, seed=42)\n",
    "test_validation_split = train_test_split[\"test\"].train_test_split(test_size=0.5, shuffle=True, seed=42)\n",
    "\n",
    "final_datasets = DatasetDict({\n",
    "    \"train\": train_test_split[\"train\"],\n",
    "    \"validation\": test_validation_split[\"train\"],\n",
    "    \"test\": test_validation_split[\"test\"]\n",
    "})\n",
    "\n",
    "print(\"Dataset carregado e dividido:\")\n",
    "print(final_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8066a568-279b-4577-98f4-342b05869295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZADOR E MODELO\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    use_fast=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    use_safetensors=True,\n",
    "    #torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "base_model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad491bc5-b131-43cc-b1c7-d9a99924227a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 17,920 || all params: 494,050,688 || trainable%: 0.0036\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82a56ba45c734cd080a8e11d4b424b9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/817 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee509d0337f14c449c0098156ac19ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27464f21c4ab41a98e7dfdf967537e55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/103 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#CONFIGURAR PROMPT TUNING\n",
    "\n",
    "peft_config = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    num_virtual_tokens=NUM_VTOK,\n",
    "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "    prompt_tuning_init_text=\"Classifique a área do Direito do texto a seguir.\",\n",
    "    tokenizer_name_or_path=MODEL_NAME\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "\n",
    "# A loss será aplicada APENAS nos tokens do rótulo.\n",
    "INSTR = \"Classifique a área do Direito do texto a seguir.\\nTexto: {texto}\\nResposta: \"\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def build_example(texto: str, label: str):\n",
    "    texto = _norm(texto)\n",
    "    label = _norm(label)\n",
    "\n",
    "    prompt = INSTR.format(texto=texto)\n",
    "\n",
    "    enc_prompt = tokenizer(prompt, truncation=True, max_length=MAX_LEN_IN, add_special_tokens=True)\n",
    "    enc_label  = tokenizer(label,  truncation=True, max_length=MAX_LEN_Y,  add_special_tokens=False)\n",
    "\n",
    "    input_ids = enc_prompt[\"input_ids\"] + enc_label[\"input_ids\"] + [tokenizer.eos_token_id]\n",
    "    attn_mask = [1] * len(input_ids)\n",
    "\n",
    "    # labels: ignora loss no prompt (=-100), calcula loss só nos tokens do rótulo + EOS\n",
    "    labels = [-100] * len(enc_prompt[\"input_ids\"]) + enc_label[\"input_ids\"] + [tokenizer.eos_token_id]\n",
    "\n",
    "    # cortes de segurança\n",
    "    max_total = MAX_LEN_IN + MAX_LEN_Y + 4\n",
    "    input_ids = input_ids[:max_total]\n",
    "    attn_mask = attn_mask[:len(input_ids)]\n",
    "    labels    = labels[:len(input_ids)]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attn_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "def preprocess(batch):\n",
    "    out = [build_example(t, a) for t, a in zip(batch[TEXT_COL], batch[LABEL_COL])]\n",
    "    return {\n",
    "        \"input_ids\": [o[\"input_ids\"] for o in out],\n",
    "        \"attention_mask\": [o[\"attention_mask\"] for o in out],\n",
    "        \"labels\": [o[\"labels\"] for o in out],\n",
    "    }\n",
    "\n",
    "cols_to_keep = [TEXT_COL, LABEL_COL]\n",
    "cols_to_remove = [c for c in final_datasets[\"train\"].column_names if c not in cols_to_keep]\n",
    "\n",
    "tokenized = {}\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    tokenized[split] = final_datasets[split].map(\n",
    "        preprocess, batched=True, remove_columns=cols_to_remove\n",
    "    )\n",
    "\n",
    "# ============================\n",
    "# DATA COLLATOR (padding)\n",
    "# ============================\n",
    "def data_collator(features):\n",
    "    batch = {}\n",
    "    keys = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "\n",
    "    for k in keys:\n",
    "        max_len = max(len(f[k]) for f in features)\n",
    "        padded = []\n",
    "        for f in features:\n",
    "            seq = f[k]\n",
    "            if k == \"labels\":\n",
    "                pad_val = -100\n",
    "            elif k == \"input_ids\":\n",
    "                pad_val = pad_id\n",
    "            else:\n",
    "                pad_val = 0\n",
    "            padded.append(seq + [pad_val] * (max_len - len(seq)))\n",
    "        dtype = torch.long\n",
    "        batch[k] = torch.tensor(padded, dtype=dtype)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7dbf83-b786-4eed-88da-778545933eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_57377/68302152.py:23: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='317' max='515' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [317/515 00:30 < 00:19, 10.40 it/s, Epoch 3.07/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.368700</td>\n",
       "      <td>0.233197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.236800</td>\n",
       "      <td>0.190247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.178100</td>\n",
       "      <td>0.120602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# EXECUTA O TREINAMENTO\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    learning_rate=LR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    weight_decay=0.0,\n",
    "    eval_strategy=\"epoch\",   # CORREÇÃO: antes estava \"eval_strategy\"\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=False,\n",
    "    overwrite_output_dir=True,\n",
    "    report_to=\"none\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=100,\n",
    "    bf16=torch.cuda.is_available(),\n",
    "    fp16=False,                    # ative se preferir e sua GPU suportar\n",
    "    logging_steps=50,\n",
    "    gradient_checkpointing=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "eval_res = trainer.evaluate()\n",
    "print(\"Resultados de validação (apenas loss):\", eval_res)\n",
    "\n",
    "# Perplexidade opcional derivada da loss\n",
    "if \"eval_loss\" in eval_res:\n",
    "    try:\n",
    "        ppl = math.exp(eval_res[\"eval_loss\"])\n",
    "        print({\"perplexity\": ppl})\n",
    "    except OverflowError:\n",
    "        pass\n",
    "\n",
    "# ============================\n",
    "# SALVAR APENAS O ADAPTADOR\n",
    "# ============================\n",
    "print(f\"Salvando adaptador de Prompt Tuning em: {OUTPUT_DIR}\")\n",
    "model.save_pretrained(f\"{OUTPUT_DIR}/prompt_adapter\")\n",
    "tokenizer.save_pretrained(f\"{OUTPUT_DIR}/tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4660ef12-7205-45fc-b62e-ecc4204fc339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predição (exemplo): Direito Civil\n"
     ]
    }
   ],
   "source": [
    "def classificar(texto: str, max_new_tokens=8):\n",
    "    instr = f\"Classifique a área do Direito do texto a seguir.\\nTexto: {texto}\\nResposta: \"\n",
    "    inputs = tokenizer(instr, return_tensors=\"pt\", truncation=True, max_length=768)\n",
    "\n",
    "    # garantir que os tensores estejam no mesmo device do modelo\n",
    "    try:\n",
    "        dev = next(model.parameters()).device\n",
    "        inputs = {k: v.to(dev) for k, v in inputs.items()}\n",
    "    except StopIteration:\n",
    "        pass\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Desative o uso de cache para contornar o bug com PEFT + DynamicCache\n",
    "    # (pode ser passado via generate e/ou no generation_config)\n",
    "    model.generation_config.use_cache = False\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=False  # <-- chave para evitar o KeyError\n",
    "        )\n",
    "    pred = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    if \"Resposta:\" in pred:\n",
    "        pred = pred.split(\"Resposta:\", 1)[-1].strip()\n",
    "    return pred\n",
    "\n",
    "#texto\"A progressão de regime prisional foi concedida ao apenado por preencher os requisitos legais.\"\n",
    "#area\"Direito Penal\"\n",
    "exemplo = \"Teoria que fala da árvore envenenada\"\n",
    "print(\"Predição (exemplo):\", classificar(exemplo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c937c38-1f92-47fc-822a-58be7b4374cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16ba48c-229e-4432-bc5e-9d0a905ef42c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PEFT_PromptTuning",
   "language": "python",
   "name": "peft_prompttuning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0265fe9b95d94c0ead97dce7a9206402": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_0952c5cb937e40c58fc8c5b8284fc43f",
       "max": 102,
       "style": "IPY_MODEL_8d16d23772e34e34934ab45108aecf03",
       "value": 102
      }
     },
     "0952c5cb937e40c58fc8c5b8284fc43f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1a49a7bd1ed345f8b99adfa8e43c5610": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_8e447a35b839428f89e212a4b11cea21",
       "max": 103,
       "style": "IPY_MODEL_3ec02e1169a7443a8ff43f24c20bc080",
       "value": 103
      }
     },
     "207319612c4043ccaec106a0074e518d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_cfb029a930a940b780fc4840ceafdcdc",
       "style": "IPY_MODEL_a98cc765c1804548ab0210fbd3ea5da5",
       "value": "Map: 100%"
      }
     },
     "26c7b6aea2274ee097fa6015c39a8662": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_dc83d86379864c868480227ad4f6a50a",
       "style": "IPY_MODEL_3d47c1bb6817425c976d782257287ddc",
       "value": " 103/103 [00:00&lt;00:00, 3093.61 examples/s]"
      }
     },
     "27464f21c4ab41a98e7dfdf967537e55": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_207319612c4043ccaec106a0074e518d",
        "IPY_MODEL_1a49a7bd1ed345f8b99adfa8e43c5610",
        "IPY_MODEL_26c7b6aea2274ee097fa6015c39a8662"
       ],
       "layout": "IPY_MODEL_7c180197040e4734ba1d0495dab49a09"
      }
     },
     "32df07a63ed04844909ef5960f0d46a4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3909755e88444cf38eaca44df11ee3f7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_cd340e6f67ff491a9e9248af33aa96c5",
       "style": "IPY_MODEL_32df07a63ed04844909ef5960f0d46a4",
       "value": "Map: 100%"
      }
     },
     "3a2d5a23c2e0478bb4fe71c7ef9c4517": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_9814dd67fb1a486f8747f47a0b716aaa",
       "max": 817,
       "style": "IPY_MODEL_c7122712970d4e45bbb88249b19f925d",
       "value": 817
      }
     },
     "3c3a7520316042f5b1d6edbb2f85a902": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3d47c1bb6817425c976d782257287ddc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3ec02e1169a7443a8ff43f24c20bc080": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "4725df6244684cc9aa30a051d936b41c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "642fd53c20c2458882a2e38a25cc3588": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_ac592039feba43a6a8a303060f03d08e",
       "style": "IPY_MODEL_6ab9bac161d84a7ca93297c94be42a16",
       "value": " 817/817 [00:00&lt;00:00, 2320.11 examples/s]"
      }
     },
     "686ca72d510c4bb88968d4bba0f569dd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6ab9bac161d84a7ca93297c94be42a16": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6b342e366a4a4d10bc0c21254148fb2e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_686ca72d510c4bb88968d4bba0f569dd",
       "style": "IPY_MODEL_cf96e4bb456b43e0ad281fb7ff8c3ff7",
       "value": " 102/102 [00:00&lt;00:00, 2944.36 examples/s]"
      }
     },
     "7c180197040e4734ba1d0495dab49a09": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "82a56ba45c734cd080a8e11d4b424b9d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_84a33b9a436e4020b4d67d7bf371157a",
        "IPY_MODEL_3a2d5a23c2e0478bb4fe71c7ef9c4517",
        "IPY_MODEL_642fd53c20c2458882a2e38a25cc3588"
       ],
       "layout": "IPY_MODEL_841e33846c2b4b3c8452fb4f75569ca6"
      }
     },
     "82a92306fea6433cba9aa082bede2dcd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "841e33846c2b4b3c8452fb4f75569ca6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "84a33b9a436e4020b4d67d7bf371157a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_82a92306fea6433cba9aa082bede2dcd",
       "style": "IPY_MODEL_4725df6244684cc9aa30a051d936b41c",
       "value": "Map: 100%"
      }
     },
     "8d16d23772e34e34934ab45108aecf03": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "8e447a35b839428f89e212a4b11cea21": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9814dd67fb1a486f8747f47a0b716aaa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a98cc765c1804548ab0210fbd3ea5da5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ac592039feba43a6a8a303060f03d08e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c7122712970d4e45bbb88249b19f925d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "cd340e6f67ff491a9e9248af33aa96c5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "cf96e4bb456b43e0ad281fb7ff8c3ff7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "cfb029a930a940b780fc4840ceafdcdc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "dc83d86379864c868480227ad4f6a50a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ee509d0337f14c449c0098156ac19ac1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_3909755e88444cf38eaca44df11ee3f7",
        "IPY_MODEL_0265fe9b95d94c0ead97dce7a9206402",
        "IPY_MODEL_6b342e366a4a4d10bc0c21254148fb2e"
       ],
       "layout": "IPY_MODEL_3c3a7520316042f5b1d6edbb2f85a902"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
