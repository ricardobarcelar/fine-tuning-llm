{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2695ba7e-cad6-4782-926e-8aad9e4d79e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRIA O AMBIENTE VIRTUAL\n",
    "\n",
    "conda create -n PEFT_PromptTuning python=3.11\n",
    "conda activate PEFT_PromptTuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3092ab4f-e403-464d-bf4d-85074f5e4f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTALA AS DEPENDENCIAS\n",
    "\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "pip install tqdm ipykernel ipywidgets transformers==4.44 datasets==2.19.1 peft==0.12.0 trl==0.9.4\n",
    "# pip installaccelerate==0.30.1 tokenizers==0.19.1 bitsandbytes==0.43.1\n",
    "\n",
    "python -m ipykernel install --user --name=PEFT_PromptTuning --display-name=\"PEFT_PromptTuning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8935f78b-71a9-42d6-953d-94735cc27f41",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# VERIFICAÇÃO DO DATASET\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"dataset_classificacao_juridica.jsonl\", lines=True)\n",
    "print(df['area'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef809d34-2685-49c8-90a2-ccbf2a3c5c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve bug com progress bar no Jupyter\n",
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
    "os.environ[\"TQDM_DISABLE\"] = \"1\"\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e48debdd-8056-43c2-b416-61410a22a4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTA AS BIBLIOTECAS\n",
    "\n",
    "import re, math, torch\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import (\n",
    "    PromptTuningConfig,\n",
    "    PromptTuningInit,\n",
    "    TaskType,\n",
    "    get_peft_model,\n",
    "    PeftModel\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4263dd8-b139-4a58-8881-dc94f34c672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURAÇÕES\n",
    "\n",
    "MODEL_NAME   = \"Qwen/Qwen2.5-0.5B\"\n",
    "DATASET_FILE = \"dataset_classificacao_juridica.jsonl\"\n",
    "OUTPUT_DIR   = \"./modelo_juridico_prompt_tuning\"\n",
    "\n",
    "TEXT_COL  = \"texto\"\n",
    "LABEL_COL = \"area\"\n",
    "\n",
    "NUM_VTOK   = 20          # nº de tokens virtuais (ajuste entre 10 e 50)\n",
    "MAX_LEN_IN = 768         # comprimento máx. do prompt\n",
    "MAX_LEN_Y  = 16          # comprimento máx. do rótulo (curto)\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS     = 5\n",
    "LR         = 5e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0a0536b-5b70-434e-9ec4-8ac209fb38b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset carregado e dividido:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['texto', 'area'],\n",
      "        num_rows: 817\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['texto', 'area'],\n",
      "        num_rows: 102\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['texto', 'area'],\n",
      "        num_rows: 103\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# CARREGAR DATASET\n",
    "full_dataset = load_dataset(\"json\", data_files={\"train\": DATASET_FILE}, split=\"train\")\n",
    "\n",
    "# split: 80/10/10\n",
    "train_test_split = full_dataset.train_test_split(test_size=0.2, shuffle=True, seed=42)\n",
    "test_validation_split = train_test_split[\"test\"].train_test_split(test_size=0.5, shuffle=True, seed=42)\n",
    "\n",
    "final_datasets = DatasetDict({\n",
    "    \"train\": train_test_split[\"train\"],\n",
    "    \"validation\": test_validation_split[\"train\"],\n",
    "    \"test\": test_validation_split[\"test\"]\n",
    "})\n",
    "\n",
    "print(\"Dataset carregado e dividido:\")\n",
    "print(final_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8066a568-279b-4577-98f4-342b05869295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZADOR E MODELO\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    use_fast=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    use_safetensors=True,\n",
    "    #torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "base_model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad491bc5-b131-43cc-b1c7-d9a99924227a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 17,920 || all params: 494,050,688 || trainable%: 0.0036\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8edf66f2bd164e229610e8c8b70723b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#CONFIGURAR PROMPT TUNING\n",
    "\n",
    "peft_config = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    num_virtual_tokens=NUM_VTOK,\n",
    "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "    prompt_tuning_init_text=\"Classifique a área do Direito do texto a seguir.\",\n",
    "    tokenizer_name_or_path=MODEL_NAME\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "\n",
    "# loss aplicada APENAS nos tokens do rótulo.\n",
    "INSTR = \"Classifique a área do Direito do texto a seguir.\\nTexto: {texto}\\nResposta: \"\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def build_example(texto: str, label: str):\n",
    "    texto = _norm(texto)\n",
    "    label = _norm(label)\n",
    "\n",
    "    prompt = INSTR.format(texto=texto)\n",
    "\n",
    "    enc_prompt = tokenizer(prompt, truncation=True, max_length=MAX_LEN_IN, add_special_tokens=True)\n",
    "    enc_label  = tokenizer(label,  truncation=True, max_length=MAX_LEN_Y,  add_special_tokens=False)\n",
    "\n",
    "    input_ids = enc_prompt[\"input_ids\"] + enc_label[\"input_ids\"] + [tokenizer.eos_token_id]\n",
    "    attn_mask = [1] * len(input_ids)\n",
    "\n",
    "    # labels: ignora loss no prompt (=-100), calcula loss só nos tokens do rótulo + EOS\n",
    "    labels = [-100] * len(enc_prompt[\"input_ids\"]) + enc_label[\"input_ids\"] + [tokenizer.eos_token_id]\n",
    "\n",
    "    # cortes de segurança\n",
    "    max_total = MAX_LEN_IN + MAX_LEN_Y + 4\n",
    "    input_ids = input_ids[:max_total]\n",
    "    attn_mask = attn_mask[:len(input_ids)]\n",
    "    labels    = labels[:len(input_ids)]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attn_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "def preprocess(batch):\n",
    "    out = [build_example(t, a) for t, a in zip(batch[TEXT_COL], batch[LABEL_COL])]\n",
    "    return {\n",
    "        \"input_ids\": [o[\"input_ids\"] for o in out],\n",
    "        \"attention_mask\": [o[\"attention_mask\"] for o in out],\n",
    "        \"labels\": [o[\"labels\"] for o in out],\n",
    "    }\n",
    "\n",
    "cols_to_keep = [TEXT_COL, LABEL_COL]\n",
    "cols_to_remove = [c for c in final_datasets[\"train\"].column_names if c not in cols_to_keep]\n",
    "\n",
    "tokenized = {}\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    tokenized[split] = final_datasets[split].map(\n",
    "        preprocess, batched=True, remove_columns=cols_to_remove\n",
    "    )\n",
    "\n",
    "def data_collator(features):\n",
    "    batch = {}\n",
    "    keys = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "\n",
    "    for k in keys:\n",
    "        max_len = max(len(f[k]) for f in features)\n",
    "        padded = []\n",
    "        for f in features:\n",
    "            seq = f[k]\n",
    "            if k == \"labels\":\n",
    "                pad_val = -100\n",
    "            elif k == \"input_ids\":\n",
    "                pad_val = pad_id\n",
    "            else:\n",
    "                pad_val = 0\n",
    "            padded.append(seq + [pad_val] * (max_len - len(seq)))\n",
    "        dtype = torch.long\n",
    "        batch[k] = torch.tensor(padded, dtype=dtype)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed7dbf83-b786-4eed-88da-778545933eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63986/658353810.py:23: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='515' max='515' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [515/515 00:47, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.368700</td>\n",
       "      <td>0.233197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.236800</td>\n",
       "      <td>0.190247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.178100</td>\n",
       "      <td>0.120602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.135800</td>\n",
       "      <td>0.097295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.120100</td>\n",
       "      <td>0.086085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de validação (apenas loss): {'eval_loss': 0.0860845223069191, 'eval_runtime': 0.3831, 'eval_samples_per_second': 266.252, 'eval_steps_per_second': 33.934, 'epoch': 5.0}\n",
      "{'perplexity': 1.089898445143001}\n",
      "Salvando adaptador de Prompt Tuning em: ./modelo_juridico_prompt_tuning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./modelo_juridico_prompt_tuning/tokenizer/tokenizer_config.json',\n",
       " './modelo_juridico_prompt_tuning/tokenizer/special_tokens_map.json',\n",
       " './modelo_juridico_prompt_tuning/tokenizer/chat_template.jinja',\n",
       " './modelo_juridico_prompt_tuning/tokenizer/vocab.json',\n",
       " './modelo_juridico_prompt_tuning/tokenizer/merges.txt',\n",
       " './modelo_juridico_prompt_tuning/tokenizer/added_tokens.json',\n",
       " './modelo_juridico_prompt_tuning/tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EXECUTA O TREINAMENTO\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    learning_rate=LR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    weight_decay=0.0,\n",
    "    eval_strategy=\"epoch\", \n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=False,\n",
    "    overwrite_output_dir=True,\n",
    "    report_to=\"none\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=100,\n",
    "    bf16=torch.cuda.is_available(),\n",
    "    fp16=False,                   \n",
    "    logging_steps=50,\n",
    "    gradient_checkpointing=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "eval_res = trainer.evaluate()\n",
    "print(\"Resultados de validação (apenas loss):\", eval_res)\n",
    "\n",
    "# Perplexidade derivada da loss\n",
    "if \"eval_loss\" in eval_res:\n",
    "    try:\n",
    "        ppl = math.exp(eval_res[\"eval_loss\"])\n",
    "        print({\"perplexity\": ppl})\n",
    "    except OverflowError:\n",
    "        pass\n",
    "\n",
    "# ============================\n",
    "# SALVAR O ADAPTADOR\n",
    "# ============================\n",
    "print(f\"Salvando adaptador de Prompt Tuning em: {OUTPUT_DIR}\")\n",
    "model.save_pretrained(f\"{OUTPUT_DIR}/prompt_adapter\")\n",
    "tokenizer.save_pretrained(f\"{OUTPUT_DIR}/tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "786f8cda-f8e9-4cc7-8f4d-903c4cd8dfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERIFICAÇÃO\n",
    "\n",
    "BASE = \"Qwen/Qwen2.5-0.5B\"\n",
    "ADAPTER_DIR = \"./modelo_juridico_prompt_tuning/prompt_adapter\"  # contém adapter_config.json e adapter_model.safetensors\n",
    "TOK_DIR = \"./modelo_juridico_prompt_tuning/tokenizer\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOK_DIR, use_fast=True, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    use_safetensors=True,\n",
    "    #torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base, ADAPTER_DIR)  # carrega .safetensors automaticamente\n",
    "model.eval()\n",
    "model.generation_config.use_cache = False  # evitar conflito com DynamicCache\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fe63235-03b6-4e59-9657-797b3ee41e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direito Penal\n"
     ]
    }
   ],
   "source": [
    "def classificar(texto: str, max_new_tokens=8):\n",
    "    instr = f\"Classifique a área do Direito do texto a seguir.\\nTexto: {texto}\\nResposta: \"\n",
    "    inputs = tokenizer(instr, return_tensors=\"pt\", truncation=True, max_length=768)\n",
    "    dev = next(model.parameters()).device\n",
    "    inputs = {k: v.to(dev) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            use_cache=False\n",
    "        )\n",
    "    pred = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    return pred.split(\"Resposta:\", 1)[-1].strip() if \"Resposta:\" in pred else pred.strip()\n",
    "\n",
    "print(classificar(\"Parte do direito que trata sobre progressão de regime prisional.\"))\n",
    "\n",
    "# Trecho do dataset que contém a resposta:\n",
    "# {texto: \"A progressão de regime prisional foi concedida ao apenado por preencher os requisitos legais.\" area: \"Direito Penal\"}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PEFT_PromptTuning",
   "language": "python",
   "name": "peft_prompttuning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "205a891bd8b4498ea7edb96f961e39ff": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "22046f9fd0e445fa9b01b2f4005121d6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_e0379be9b2c44906a4a3cc3e115f0284",
       "style": "IPY_MODEL_595d70b8da8d481f954f62a521b03e32",
       "value": "Map: 100%"
      }
     },
     "2ae742cca5c648af9b03d42356dd3ec7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "595d70b8da8d481f954f62a521b03e32": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "80fdc5f20e664bf2b5681470daa45cb8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "8edf66f2bd164e229610e8c8b70723b2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_22046f9fd0e445fa9b01b2f4005121d6",
        "IPY_MODEL_afbc568046f7466e8cdaa967bf1126be",
        "IPY_MODEL_9d111c08c6df4c278f2bbfe4be765dd1"
       ],
       "layout": "IPY_MODEL_2ae742cca5c648af9b03d42356dd3ec7"
      }
     },
     "9d111c08c6df4c278f2bbfe4be765dd1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_205a891bd8b4498ea7edb96f961e39ff",
       "style": "IPY_MODEL_9e5abf34e06e46fda837b352e976549b",
       "value": " 102/102 [00:00&lt;00:00, 504.37 examples/s]"
      }
     },
     "9e5abf34e06e46fda837b352e976549b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "afbc568046f7466e8cdaa967bf1126be": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_ce7edc695ae24b9988f7a515c6286cb6",
       "max": 102,
       "style": "IPY_MODEL_80fdc5f20e664bf2b5681470daa45cb8",
       "value": 102
      }
     },
     "ce7edc695ae24b9988f7a515c6286cb6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e0379be9b2c44906a4a3cc3e115f0284": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
